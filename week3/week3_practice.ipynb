{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Filtering & Frequency Domain — Lab Practice\n",
    "\n",
    "**Topics:** Image Resizing, Convolution, Smoothing, Frequency Domain & Sharpening\n",
    "\n",
    "This notebook accompanies the Week 3 lecture slides. We will resize images with different interpolation methods, implement 2D convolution from scratch, apply smoothing and sharpening filters, and visualize the frequency domain — all hands-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom scipy.ndimage import correlate\n\n%matplotlib inline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "This notebook works both **locally** and on **Google Colab**.\n",
    "- **Local**: images are loaded from the repository’s `images/` folder.\n",
    "- **Colab**: images are automatically downloaded from GitHub on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, urllib.request\n\n# Detect Google Colab\nIN_COLAB = \"google.colab\" in str(get_ipython()) if hasattr(__builtins__, \"__IPYTHON__\") else False\n\n# Image paths — we use a 256×256 version for faster processing\nREPO_URL = \"https://raw.githubusercontent.com/HyeongminLEE/image-processing-tutorial/main\"\nIMAGE_DIR = \"images\"\nCOLOR_NAME = \"parrots_256.jpg\"\nGRAY_NAME = \"parrots_256_gray.jpg\"\n\nif IN_COLAB:\n    os.makedirs(IMAGE_DIR, exist_ok=True)\n    for name in [COLOR_NAME, GRAY_NAME]:\n        url = f\"{REPO_URL}/{IMAGE_DIR}/{name}\"\n        dest = os.path.join(IMAGE_DIR, name)\n        if not os.path.exists(dest):\n            print(f\"Downloading {name} ...\")\n            urllib.request.urlretrieve(url, dest)\n    IMAGE_PATH = os.path.join(IMAGE_DIR, COLOR_NAME)\n    GRAY_PATH = os.path.join(IMAGE_DIR, GRAY_NAME)\nelse:\n    IMAGE_PATH = os.path.join(\"..\", IMAGE_DIR, COLOR_NAME)\n    GRAY_PATH = os.path.join(\"..\", IMAGE_DIR, GRAY_NAME)\n\nprint(f\"Image path: {IMAGE_PATH}\")\nprint(f\"Running on: {'Google Colab' if IN_COLAB else 'Local'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Display helpers\n\nWe define utility functions used throughout this notebook. This week adds `show_kernel` for visualizing filter weights and `show_spectrum` for displaying magnitude spectra, alongside the familiar `show_images`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def show_images(*imgs, titles=None, scale=4):\n    \"\"\"Display images in a single row.\n\n    Grayscale (2-D) arrays automatically use a gray colormap.\n    *titles* is an optional list of strings, one per image.\n    \"\"\"\n    n = len(imgs)\n    fig, axes = plt.subplots(1, n, figsize=(scale * n, scale))\n    if n == 1:\n        axes = [axes]\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        if img.ndim == 2:\n            ax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n        else:\n            ax.imshow(img)\n        if titles:\n            ax.set_title(titles[i])\n        ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_proportional(*imgs, titles=None, scale=6):\n    \"\"\"Display images vertically, each at its proportional size.\n\n    *scale* sets the width (in inches) of the widest image.\n    \"\"\"\n    n = len(imgs)\n    max_w = max(img.shape[1] for img in imgs)\n    px_per_inch = max_w / scale\n    heights = [img.shape[0] / px_per_inch for img in imgs]\n    gap = 0.35\n    total_h = sum(heights) + gap * (n - 1)\n\n    fig, axes = plt.subplots(n, 1, figsize=(scale, total_h),\n                             gridspec_kw={\"height_ratios\": [img.shape[0] for img in imgs]})\n    if n == 1:\n        axes = [axes]\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        ax.set_anchor(\"W\")\n        if img.ndim == 2:\n            ax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n        else:\n            ax.imshow(img)\n        if titles:\n            ax.set_title(titles[i], loc=\"left\")\n        ax.axis(\"off\")\n    fig.subplots_adjust(hspace=gap * n / sum(heights))\n    plt.show()\n\n\ndef show_kernel(kernel, title=\"Kernel\", ax=None):\n    \"\"\"Display a small kernel as an annotated heatmap.\"\"\"\n    standalone = ax is None\n    if standalone:\n        fig, ax = plt.subplots(figsize=(3, 3))\n    vmax = np.max(np.abs(kernel))\n    ax.imshow(kernel, cmap=\"coolwarm\", vmin=-vmax, vmax=vmax)\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            val = kernel[i, j]\n            text = f\"{val:.2f}\" if val != int(val) else str(int(val))\n            ax.text(j, i, text, ha=\"center\", va=\"center\", fontsize=10)\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if standalone:\n        plt.tight_layout()\n        plt.show()\n\n\ndef show_kernels(*kernels, titles=None):\n    \"\"\"Display multiple kernels as annotated heatmaps in a row.\"\"\"\n    n = len(kernels)\n    fig, axes = plt.subplots(1, n, figsize=(3 * n, 3))\n    if n == 1:\n        axes = [axes]\n    for i, (ax, k) in enumerate(zip(axes, kernels)):\n        show_kernel(k, title=titles[i] if titles else \"Kernel\", ax=ax)\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_kernel_3d(kernel, title=\"\"):\n    \"\"\"Display a kernel as a 3D surface plot.\"\"\"\n    k = kernel.shape[0] // 2\n    ax_range = np.arange(-k, k + 1)\n    xx, yy = np.meshgrid(ax_range, ax_range)\n    fig = plt.figure(figsize=(6, 5))\n    ax = fig.add_subplot(111, projection=\"3d\")\n    ax.plot_surface(xx, yy, kernel, cmap=\"viridis\")\n    if title:\n        ax.set_title(title)\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_maps(*maps, titles=None, scale=4):\n    \"\"\"Display 2D float arrays (masks, spectra, etc.) with auto grayscale mapping.\"\"\"\n    n = len(maps)\n    fig, axes = plt.subplots(1, n, figsize=(scale * n, scale))\n    if n == 1:\n        axes = [axes]\n    for i, (ax, m) in enumerate(zip(axes, maps)):\n        ax.imshow(m, cmap=\"gray\")\n        if titles:\n            ax.set_title(titles[i])\n        ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_spectrum(img, spectrum, title=\"\"):\n    \"\"\"Display a grayscale image and its magnitude spectrum side by side.\n\n    *spectrum* is a pre-computed 2D array (e.g., from log_magnitude_spectrum).\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    axes[0].imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n    axes[0].set_title(f\"{title} (Spatial)\" if title else \"Spatial domain\")\n    axes[0].axis(\"off\")\n    axes[1].imshow(spectrum, cmap=\"gray\")\n    axes[1].set_title(f\"{title} (Spectrum)\" if title else \"Magnitude spectrum\")\n    axes[1].axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_freq_response(responses, labels, xlim=None):\n    \"\"\"Plot 1D frequency response cross-sections through center.\"\"\"\n    fig, ax = plt.subplots(figsize=(6, 4))\n    for resp, label in zip(responses, labels):\n        center = resp.shape[0] // 2\n        freq_axis = np.arange(resp.shape[1]) - resp.shape[1] // 2\n        ax.plot(freq_axis, resp[center, :], label=label)\n    ax.set_title(\"Frequency response (1D cross-section)\")\n    ax.set_xlabel(\"Frequency\")\n    ax.set_ylabel(\"Magnitude\")\n    ax.legend()\n    if xlim:\n        ax.set_xlim(xlim)\n    plt.tight_layout()\n    plt.show()\n\n\ndef rescale_for_display(img):\n    \"\"\"Rescale a float image to [0, 255] uint8 for display.\"\"\"\n    lo, hi = img.min(), img.max()\n    return ((img - lo) / (hi - lo) * 255).astype(np.uint8)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Image Resizing & Interpolation\n",
    "\n",
    "When we change an image’s dimensions, pixels must be **created** (upsampling) or **discarded** (downsampling). The interpolation method determines how new pixel values are estimated from existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_color = np.array(Image.open(IMAGE_PATH))\n",
    "img_gray = np.array(Image.open(GRAY_PATH))\n",
    "\n",
    "print(f\"Color \\u2014 shape: {img_color.shape}, dtype: {img_color.dtype}\")\n",
    "print(f\"Gray  \\u2014 shape: {img_gray.shape}, dtype: {img_gray.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Downsampling by pixel skipping\n\nThe simplest downsampling: take every $N$th pixel and discard the rest. This is just NumPy slicing: `img[::N, ::N]`.\n\nEach image below is displayed at its **actual relative size** so you can see how much information is lost."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "factors = [1, 2, 4]\ndownsampled = [img_gray[::f, ::f] for f in factors]\ntitles = [f\"{d.shape[0]}×{d.shape[1]}\" if f == 1\n          else f\"{d.shape[0]}×{d.shape[1]} (1/{f})\"\n          for f, d in zip(factors, downsampled)]\n\nshow_proportional(*downsampled, titles=titles)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling with different interpolation methods\n",
    "\n",
    "To upsample, we must **estimate** new pixel values. Three common methods:\n",
    "- **Nearest neighbor**: copy the closest pixel — fast but blocky\n",
    "- **Bilinear**: weighted average of 4 neighbors — smooth but slightly blurry\n",
    "- **Bicubic**: cubic polynomial through 16 neighbors — sharpest, most common default\n",
    "\n",
    "Let’s downsample to a small size, then upsample back to see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pil_gray = Image.open(GRAY_PATH)\nH, W = img_gray.shape\n\n# Downsample to 32×32\nsmall = pil_gray.resize((32, 32), Image.NEAREST)\n\n# Upsample back with different methods\nmethods = {\"Nearest\": Image.NEAREST, \"Bilinear\": Image.BILINEAR, \"Bicubic\": Image.BICUBIC}\nupsampled = {name: np.array(small.resize((W, H), method)) for name, method in methods.items()}\n\nshow_images(img_gray, upsampled[\"Nearest\"], upsampled[\"Bilinear\"], upsampled[\"Bicubic\"],\n            titles=[\"Original\", \"Nearest\", \"Bilinear\", \"Bicubic\"])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s zoom in on a small region to see the interpolation artifacts more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "r1, r2, c1, c2 = 20, 90, 20, 90\ncrops = [img_gray[r1:r2, c1:c2]] + [upsampled[m][r1:r2, c1:c2] for m in methods]\n\nshow_images(*crops, titles=[\"Original\"] + list(methods.keys()), scale=3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Spatial Filtering & Convolution\n\nIn Week 2, we transformed each pixel **independently** (point processing: $g(x,y) = T[f(x,y)]$). Now we use **neighborhoods**: the output at each pixel depends on the pixel *and its surrounding region*.\n\nThe core operation is **convolution**: slide a small weight matrix (kernel) over the image, computing a weighted sum at each position.\n\n$$g(x,y) = \\sum_{s=-k}^{k}\\sum_{t=-k}^{k} h(s,t) \\cdot f(x+s,\\, y+t)$$\n\n> **Note:** Strictly speaking, this formula is *correlation*. True mathematical convolution flips the kernel first. In practice, all standard libraries (`scipy`, `PyTorch`, etc.) compute correlation and call it \"convolution\" — we follow that convention here."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2D convolution: step by step\n\nLet's work through a small example: a 5×5 \"image\" convolved with a 3×3 kernel."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_img = np.array([\n",
    "    [1,  2,  3,  4,  5],\n",
    "    [6,  7,  8,  9,  10],\n",
    "    [11, 12, 13, 14, 15],\n",
    "    [16, 17, 18, 19, 20],\n",
    "    [21, 22, 23, 24, 25]\n",
    "], dtype=np.float64)\n",
    "\n",
    "kernel_2d = np.array([\n",
    "    [0, -1,  0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1,  0]\n",
    "], dtype=np.float64)\n",
    "\n",
    "print(f\"Image (5\\u00d75):\\n{small_img.astype(int)}\")\n",
    "print()\n",
    "show_kernel(kernel_2d, title=\"Kernel (3\\u00d73)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's compute one output pixel explicitly: extract the 3×3 neighborhood, multiply element-wise with the kernel, and sum."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute output at position (1, 1) — kernel centered at row=1, col=1\nrow, col = 1, 1\nneighborhood = small_img[row:row + 3, col:col + 3]\nproducts = neighborhood * kernel_2d\n\nprint(f\"Neighborhood at ({row},{col}):\\n{neighborhood.astype(int)}\")\nprint(f\"\\nKernel:\\n{kernel_2d.astype(int)}\")\nprint(f\"\\nElement-wise products:\\n{products.astype(int)}\")\nprint(f\"\\nSum = {products.sum():.0f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing 2D convolution from scratch\n",
    "\n",
    "Let’s write a full convolution function. This builds deep intuition for what every library function does under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def conv2d(image, kernel, mode=\"same\"):\n    \"\"\"2D convolution of a grayscale image with a kernel.\n\n    Parameters\n    ----------\n    image  : 2D array (H, W)\n    kernel : 2D array (kH, kW), must be odd-sized\n    mode   : \"valid\" or \"same\"\n    \"\"\"\n    img = image.astype(np.float64)\n    k = kernel.astype(np.float64)\n    kH, kW = k.shape\n\n    if mode == \"same\":\n        pH, pW = kH // 2, kW // 2\n        img = np.pad(img, ((pH, pH), (pW, pW)), mode=\"constant\", constant_values=0)\n    elif mode != \"valid\":\n        raise ValueError(f\"Unknown mode: {mode}\")\n\n    H, W = img.shape\n    outH, outW = H - kH + 1, W - kW + 1\n    output = np.zeros((outH, outW), dtype=np.float64)\n\n    for i in range(outH):\n        for j in range(outW):\n            output[i, j] = np.sum(img[i:i + kH, j:j + kW] * k)\n\n    return output"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_valid = conv2d(small_img, kernel_2d, mode=\"valid\")\n",
    "result_same = conv2d(small_img, kernel_2d, mode=\"same\")\n",
    "\n",
    "print(f\"Input shape:        {small_img.shape}\")\n",
    "print(f\"Kernel shape:       {kernel_2d.shape}\")\n",
    "print(f\"Valid output shape: {result_valid.shape}  (5 - 3 + 1 = 3)\")\n",
    "print(f\"Same output shape:  {result_same.shape}   (padded to match input)\")\n",
    "print(f\"\\nValid output:\\n{result_valid.astype(int)}\")\n",
    "print(f\"\\nSame output:\\n{result_same.astype(int)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Verification against scipy\n\nLet's verify our implementation matches `scipy.ndimage.correlate`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "ours = conv2d(small_img, kernel_2d, mode=\"same\")\nscipy_result = correlate(small_img, kernel_2d, mode=\"constant\", cval=0)\n\nprint(f\"Our conv2d:\\n{ours.astype(int)}\")\nprint(f\"\\nscipy correlate:\\n{scipy_result.astype(int)}\")\nprint(f\"\\nMatch: {np.allclose(ours, scipy_result)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Applying convolution to a real image\n\nLet's apply the sharpening kernel to the grayscale parrot image.\n\n> Since our from-scratch implementation uses Python loops, it is **slow** on full-size images. For the rest of this notebook we use `scipy.ndimage.correlate` for efficiency."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpened = correlate(img_gray.astype(np.float64), kernel_2d, mode=\"constant\", cval=0)\n",
    "sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "\n",
    "show_images(img_gray, sharpened, titles=[\"Original\", \"Sharpened (kernel from above)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output size\n",
    "\n",
    "The output dimensions depend on the padding mode:\n",
    "\n",
    "| Mode | Padding $p$ | Output size |\n",
    "|:---:|:---:|:---:|\n",
    "| **Valid** | $0$ | $H - F + 1$ |\n",
    "| **Same** | $\\lfloor F/2 \\rfloor$ | $H$ |\n",
    "| **Full** | $F - 1$ | $H + F - 1$ |\n",
    "\n",
    "General formula: $O = H + 2p - F + 1$, where $F$ is the kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "H, W = img_gray.shape\nF = 5\ntest_kernel = np.ones((F, F)) / (F * F)\n\nvalid_out = conv2d(img_gray, test_kernel, mode=\"valid\")\nsame_out  = conv2d(img_gray, test_kernel, mode=\"same\")\n\nprint(f\"Input:  {img_gray.shape}\")\nprint(f\"Kernel: {test_kernel.shape}\")\nprint(f\"Valid:  {valid_out.shape}  (expected {H - F + 1}×{W - F + 1})\")\nprint(f\"Same:   {same_out.shape}  (expected {H}×{W})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercises\n\n**Exercise 2.1:** Use `scipy.ndimage.correlate` to apply the **identity kernel** $\\begin{bmatrix}0&0&0\\\\0&1&0\\\\0&0&0\\end{bmatrix}$ to the grayscale image (`img_gray`). Display the original and result side by side.\n\nThen try a **shift kernel** — place the `1` at a different position in the 3×3 matrix (e.g., bottom-right corner) and observe the effect. What happens to the image, and why?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n\n\n# What happens to the image when you apply the shift kernel? Why?\n# (You may write your answer in Korean.)\n# "
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Smoothing Filters\n",
    "\n",
    "Smoothing filters blur the image by averaging each pixel with its neighbors. They are **low-pass** filters: they preserve gradual intensity changes (low frequencies) and suppress rapid changes like edges and noise (high frequencies).\n",
    "\n",
    "Two main types:\n",
    "- **Box filter**: all weights equal (simple average)\n",
    "- **Gaussian filter**: closer neighbors get higher weight (bell curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box filter\n",
    "\n",
    "The simplest smoothing kernel: every weight is $\\frac{1}{N^2}$. A 3×3 box filter:\n",
    "\n",
    "$$h = \\frac{1}{9}\\begin{bmatrix}1 & 1 & 1\\\\1 & 1 & 1\\\\1 & 1 & 1\\end{bmatrix}$$\n",
    "\n",
    "Each output pixel becomes the **average** of its $3 \\times 3$ neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_box_kernel(size):\n",
    "    \"\"\"Create a box (mean) kernel of the given size.\"\"\"\n",
    "    return np.ones((size, size), dtype=np.float64) / (size * size)\n",
    "\n",
    "box3 = make_box_kernel(3)\n",
    "show_kernel(box3, title=\"3\\u00d73 Box kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [3, 7, 15, 31]\n",
    "blurred = []\n",
    "for s in sizes:\n",
    "    k = make_box_kernel(s)\n",
    "    b = correlate(img_gray.astype(np.float64), k, mode=\"constant\", cval=0)\n",
    "    blurred.append(np.clip(b, 0, 255).astype(np.uint8))\n",
    "\n",
    "show_images(img_gray, *blurred,\n",
    "            titles=[\"Original\"] + [f\"Box {s}\\u00d7{s}\" for s in sizes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian filter\n",
    "\n",
    "Instead of equal weights, a Gaussian kernel gives **higher weight to closer neighbors**:\n",
    "\n",
    "$$h(s,t) = \\frac{1}{2\\pi\\sigma^2}\\exp\\!\\left(-\\frac{s^2+t^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "The parameter $\\sigma$ controls the spread: larger $\\sigma$ = wider kernel = more smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gaussian_kernel(size, sigma):\n",
    "    \"\"\"Create a 2D Gaussian kernel of the given size and sigma.\"\"\"\n",
    "    k = size // 2\n",
    "    ax = np.arange(-k, k + 1, dtype=np.float64)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    kernel = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    kernel /= kernel.sum()\n",
    "    return kernel\n",
    "\n",
    "g5 = make_gaussian_kernel(5, sigma=1.0)\n",
    "print(f\"Kernel shape: {g5.shape}\")\n",
    "print(f\"Sum of weights: {g5.sum():.6f}\")\n",
    "show_kernel(g5, title=\"5\\u00d75 Gaussian (\\u03c3=1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "g15 = make_gaussian_kernel(15, sigma=3.0)\nshow_kernel_3d(g15, title=\"Gaussian kernel (\\u03c3=3.0)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [1, 3, 5, 10]\n",
    "gaussian_blurred = []\n",
    "for sigma in sigmas:\n",
    "    size = int(6 * sigma + 1) | 1   # ensure odd size, ~6\\u03c3 wide\n",
    "    k = make_gaussian_kernel(size, sigma)\n",
    "    b = correlate(img_gray.astype(np.float64), k, mode=\"constant\", cval=0)\n",
    "    gaussian_blurred.append(np.clip(b, 0, 255).astype(np.uint8))\n",
    "\n",
    "show_images(img_gray, *gaussian_blurred,\n",
    "            titles=[\"Original\"] + [f\"Gaussian \\u03c3={s}\" for s in sigmas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box vs Gaussian comparison\n",
    "\n",
    "At similar kernel sizes, Gaussian produces a **smoother, more natural** blur because nearby pixels contribute more than distant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_result = correlate(img_gray.astype(np.float64), make_box_kernel(15), mode=\"constant\", cval=0)\n",
    "gauss_result = correlate(img_gray.astype(np.float64), make_gaussian_kernel(15, 3.0), mode=\"constant\", cval=0)\n",
    "\n",
    "box_result = np.clip(box_result, 0, 255).astype(np.uint8)\n",
    "gauss_result = np.clip(gauss_result, 0, 255).astype(np.uint8)\n",
    "\n",
    "show_images(img_gray, box_result, gauss_result,\n",
    "            titles=[\"Original\", \"Box 15\\u00d715\", \"Gaussian 15\\u00d715 (\\u03c3=3)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercises\n\n**Exercise 3.1:** Apply a 7×7 box filter to the color image (`img_color`). Since `correlate` works on 2D arrays, you need to process each R, G, B channel separately and recombine.\n\n*Hint:* Use `make_box_kernel(7)` and `np.stack` to merge the channels back."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Exercise 3.2:** Smooth the grayscale image (`img_gray`) with a Gaussian filter ($\\sigma=3$), then compute the **difference** between the original and the smoothed image. This difference is the **high-frequency detail** that smoothing removes.\n\nDisplay three images: original, smoothed, and the difference.\n\n*Hint:* The difference image will have negative values — you need to rescale it to [0, 255] for display. Think about how min-max stretching from Week 2 can help here."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Frequency Domain & 2D FFT\n",
    "\n",
    "The **Convolution Theorem** says: convolution in the spatial domain equals **multiplication** in the frequency domain.\n",
    "\n",
    "$$f * h \\;\\longleftrightarrow\\; F \\cdot H$$\n",
    "\n",
    "The **2D Discrete Fourier Transform (DFT)** decomposes an image into its constituent spatial frequencies. Low frequencies represent smooth, gradual changes; high frequencies represent edges and fine detail.\n",
    "\n",
    "Key NumPy functions:\n",
    "- `np.fft.fft2(img)` — compute the 2D DFT\n",
    "- `np.fft.fftshift(F)` — move the DC component (average brightness) to the center\n",
    "- `np.fft.ifft2(F)` — inverse DFT (back to spatial domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude spectrum\n",
    "\n",
    "To visualize the frequency content, we plot the **log-magnitude** of the DFT: $\\log(1 + |F(u,v)|)$.\n",
    "\n",
    "The log scaling compresses the huge dynamic range so that both the bright DC component and the faint high-frequency components are visible."
   ]
  },
  {
   "cell_type": "code",
   "source": "def log_magnitude_spectrum(img):\n    \"\"\"Compute the centered log-magnitude spectrum of a grayscale image.\n\n    Steps:\n    1. 2D FFT            \\u2192 np.fft.fft2\n    2. Shift DC to center \\u2192 np.fft.fftshift\n    3. Log-compress       \\u2192 log(1 + |F|)\n    \"\"\"\n    F = np.fft.fft2(img.astype(np.float64))\n    F_shifted = np.fft.fftshift(F)\n    return np.log1p(np.abs(F_shifted))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "show_spectrum(img_gray, log_magnitude_spectrum(img_gray), title=\"Grayscale parrot\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### How image content maps to frequency content\n\nSmooth regions concentrate energy at the **center** (low frequencies), while edges and textures spread energy **outward** (high frequencies). Let's compare the original with a heavily blurred version."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "smooth = correlate(img_gray.astype(np.float64),\n                   make_gaussian_kernel(31, 5.0), mode=\"constant\", cval=0)\nsmooth = np.clip(smooth, 0, 255).astype(np.uint8)\n\nshow_spectrum(img_gray, log_magnitude_spectrum(img_gray), title=\"Original\")\nshow_spectrum(smooth, log_magnitude_spectrum(smooth), title=\"Heavily smoothed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-pass filtering in frequency domain\n",
    "\n",
    "We can blur an image directly in the frequency domain:\n",
    "1. Compute 2D DFT\n",
    "2. Multiply by a **low-pass mask** (keeps center, blocks edges)\n",
    "3. Inverse DFT back to spatial domain\n",
    "\n",
    "The result is equivalent to spatial convolution with a smoothing filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute 2D DFT\nF = np.fft.fft2(img_gray.astype(np.float64))\nF_shifted = np.fft.fftshift(F)\n\n# Create a circular low-pass mask\nH, W = img_gray.shape\ncy, cx = H // 2, W // 2\nY, X = np.ogrid[:H, :W]\nradius = 30\nmask_lp = ((X - cx)**2 + (Y - cy)**2 <= radius**2).astype(np.float64)\n\n# Apply mask and inverse FFT\nF_filtered = F_shifted * mask_lp\nresult_lp = np.fft.ifft2(np.fft.ifftshift(F_filtered)).real\nresult_lp = np.clip(result_lp, 0, 255).astype(np.uint8)\n\nshow_maps(mask_lp, titles=[f\"Low-pass mask (r={radius})\"])\nshow_images(img_gray, result_lp, titles=[\"Original\", \"Low-pass filtered\"])\nshow_spectrum(img_gray, log_magnitude_spectrum(img_gray), title=\"Original\")\nshow_spectrum(result_lp, log_magnitude_spectrum(result_lp), title=\"Low-pass filtered\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-pass filtering in frequency domain\n",
    "\n",
    "The complement of low-pass is high-pass: $H_{\\text{HP}} = 1 - H_{\\text{LP}}$\n",
    "\n",
    "High-pass filtering keeps edges and fine detail while removing smooth regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "mask_hp = 1 - mask_lp\n\nF_hp = F_shifted * mask_hp\nresult_hp = np.fft.ifft2(np.fft.ifftshift(F_hp)).real\nresult_hp_display = rescale_for_display(result_hp)\n\nshow_maps(mask_hp, titles=[\"High-pass mask\"])\nshow_images(img_gray, result_hp_display, titles=[\"Original\", \"High-pass filtered (edges)\"])\nshow_spectrum(img_gray, log_magnitude_spectrum(img_gray), title=\"Original\")\nshow_spectrum(result_hp_display, log_magnitude_spectrum(result_hp_display), title=\"High-pass filtered\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Exercises\n\n**Exercise 4.1:** Experiment with different **low-pass cutoff radii** (5, 15, 30, 60) on the grayscale image (`img_gray`). For each radius, create a circular mask, apply it in the frequency domain, and display the result. Show all four filtered images in a single row.\n\n*Hint:* Reuse the FFT and mask construction pattern from the demo above, changing only the `radius`.\n\nWhat happens as the radius increases? How does this relate to the kernel size of spatial smoothing?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n\n\n# What happens as the radius increases? How does it relate to the kernel size of spatial smoothing?\n# (You may write your answer in Korean.)\n# "
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Exercise 4.2:** Explore what happens when you **boost a single frequency component** in the spectrum.\n\n1. Compute the FFT of `img_gray` (use `np.fft.fft2` and `np.fft.fftshift`)\n2. Add a large value (e.g., `500000`) at position `(50, 50)` and its symmetric counterpart `(H-50, W-50)` — this keeps the result real-valued\n3. Reconstruct the image with `np.fft.ifftshift` → `np.fft.ifft2`, take the real part, and clip to [0, 255]\n4. Display the original and modified images side by side\n\nTry a few different locations (vertical offset, horizontal offset, diagonal) and distances from center. How does the position of the boosted point relate to the wave pattern you see?\n\n*Hint:* The symmetric counterpart ensures the result stays real-valued. For a point at `(cy - dy, cx - dx)`, its counterpart is `(cy + dy, cx + dx)`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n\n\n# How does the point's position relate to the direction and frequency of the wave?\n# (You may write your answer in Korean.)\n# "
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Sharpening Filters\n\nSharpening enhances edges and fine detail. The key insight:\n\n$$\\text{High-pass} = \\text{Original} - \\text{Low-pass (blurred)}$$\n\nTherefore:\n\n$$\\text{Sharpened} = \\text{Original} + \\alpha \\cdot \\text{High-pass}$$\n\nTwo common approaches:\n1. **Laplacian**: a single kernel that directly computes the second derivative (edges)\n2. **Unsharp masking**: blur the image, subtract the blur to get edges, add them back"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Laplacian filter\n\nThe Laplacian kernel detects edges by computing the **second derivative** of the image. Its weights sum to **zero**, so flat regions produce zero output — only intensity changes survive.\n\nTwo variants:\n- **4-connected**: $\\begin{bmatrix}0 & 1 & 0\\\\1 & -4 & 1\\\\0 & 1 & 0\\end{bmatrix}$ — horizontal & vertical edges\n- **8-connected**: $\\begin{bmatrix}1 & 1 & 1\\\\1 & -8 & 1\\\\1 & 1 & 1\\end{bmatrix}$ — includes diagonal edges"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "lap4 = np.array([[0,  1, 0],\n                 [1, -4, 1],\n                 [0,  1, 0]], dtype=np.float64)\n\nlap8 = np.array([[1,  1, 1],\n                 [1, -8, 1],\n                 [1,  1, 1]], dtype=np.float64)\n\nshow_kernels(lap4, lap8, titles=[\"Laplacian (4-connected)\", \"Laplacian (8-connected)\"])\n\nprint(f\"4-connected sum: {lap4.sum():.0f}\")\nprint(f\"8-connected sum: {lap8.sum():.0f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "edges4 = correlate(img_gray.astype(np.float64), lap4, mode=\"constant\", cval=0)\nedges8 = correlate(img_gray.astype(np.float64), lap8, mode=\"constant\", cval=0)\n\nshow_images(img_gray, rescale_for_display(edges4), rescale_for_display(edges8),\n            titles=[\"Original\", \"Laplacian 4-connected\", \"Laplacian 8-connected\"])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpening with the Laplacian\n",
    "\n",
    "To sharpen, **subtract** the Laplacian (edge map) from the original:\n",
    "\n",
    "$$\\text{Sharpened} = \\text{Original} - \\text{Laplacian}$$\n",
    "\n",
    "(We subtract because the standard Laplacian has a **negative** center weight. Equivalently, some formulations negate the kernel and add instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpened_lap = img_gray.astype(np.float64) - edges4\n",
    "sharpened_lap = np.clip(sharpened_lap, 0, 255).astype(np.uint8)\n",
    "\n",
    "show_images(img_gray, rescale_for_display(edges4), sharpened_lap,\n",
    "            titles=[\"Original\", \"Laplacian edges\", \"Sharpened (orig \\u2212 Laplacian)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsharp masking\n",
    "\n",
    "A more flexible sharpening method with a tunable strength parameter $\\alpha$:\n",
    "\n",
    "$$\\text{Sharpened} = \\text{Original} + \\alpha \\cdot (\\text{Original} - \\text{Blurred})$$\n",
    "\n",
    "The term $(\\text{Original} - \\text{Blurred})$ extracts the high-frequency detail. Adding it back at strength $\\alpha$ enhances edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsharp_mask(img, sigma=3.0, alpha=1.0):\n",
    "    \"\"\"Apply unsharp masking to a grayscale image.\"\"\"\n",
    "    size = int(6 * sigma + 1) | 1\n",
    "    kernel = make_gaussian_kernel(size, sigma)\n",
    "    blurred = correlate(img.astype(np.float64), kernel, mode=\"constant\", cval=0)\n",
    "    detail = img.astype(np.float64) - blurred\n",
    "    sharpened = img.astype(np.float64) + alpha * detail\n",
    "    return np.clip(sharpened, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.5, 1.0, 2.0, 4.0]\n",
    "results = [unsharp_mask(img_gray, sigma=3.0, alpha=a) for a in alphas]\n",
    "\n",
    "show_images(img_gray, *results,\n",
    "            titles=[\"Original\"] + [f\"\\u03b1 = {a}\" for a in alphas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also visualize the **detail** (high-pass) image that unsharp masking extracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(6 * 3.0 + 1) | 1\n",
    "k = make_gaussian_kernel(size, 3.0)\n",
    "blurred_for_detail = correlate(img_gray.astype(np.float64), k, mode=\"constant\", cval=0)\n",
    "detail = img_gray.astype(np.float64) - blurred_for_detail\n",
    "\n",
    "show_images(img_gray,\n",
    "            np.clip(blurred_for_detail, 0, 255).astype(np.uint8),\n",
    "            rescale_for_display(detail),\n",
    "            titles=[\"Original\", \"Blurred (low-pass)\", \"Detail (high-pass)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercises\n\n**Exercise 5.1:** Build a simple **image enhancement pipeline** on the grayscale image (`img_gray`) that combines Weeks 2 and 3:\n1. Apply **gamma correction** ($\\gamma = 0.7$) to brighten dark regions\n2. Apply **unsharp masking** ($\\sigma=2, \\alpha=1.0$) to sharpen\n\nDisplay three images side by side: Original, After gamma, After gamma + sharpening."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "**Exercise 5.2:** Investigate what happens when you sharpen a **noisy** image.\n\n1. Add Gaussian noise to `img_gray`: generate random values with `np.random.normal(0, 20, img_gray.shape)`, add to the image, and clip to [0, 255]\n2. Apply unsharp masking ($\\sigma=2, \\alpha=2.0$) to both the **clean** and **noisy** images\n3. Display four images: clean original, sharpened clean, noisy original, sharpened noisy\n\nWhat happens to the noise after sharpening? Why does this occur?\n\n*Hint:* Think about what frequency range noise occupies, and what sharpening does in the frequency domain."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n\n\n# What happens to the noise after sharpening? Why does this occur?\n# (You may write your answer in Korean.)\n# "
  }
 ]
}