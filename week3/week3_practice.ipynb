{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Filtering & Frequency Domain — Lab Practice\n",
    "\n",
    "**Topics:** Image Resizing, Convolution, Smoothing, Frequency Domain & Sharpening\n",
    "\n",
    "This notebook accompanies the Week 3 lecture slides. We will resize images with different interpolation methods, implement 2D convolution from scratch, apply smoothing and sharpening filters, and visualize the frequency domain — all hands-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.ndimage import correlate\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "This notebook works both **locally** and on **Google Colab**.\n",
    "- **Local**: images are loaded from the repository’s `images/` folder.\n",
    "- **Colab**: images are automatically downloaded from GitHub on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "\n",
    "# Detect Google Colab\n",
    "IN_COLAB = \"google.colab\" in str(get_ipython()) if hasattr(__builtins__, \"__IPYTHON__\") else False\n",
    "\n",
    "# Image paths\n",
    "REPO_URL = \"https://raw.githubusercontent.com/HyeongminLEE/image-processing-tutorial/main\"\n",
    "IMAGE_DIR = \"images\"\n",
    "IMAGE_NAME = \"parrots_square.jpg\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "    url = f\"{REPO_URL}/{IMAGE_DIR}/{IMAGE_NAME}\"\n",
    "    dest = os.path.join(IMAGE_DIR, IMAGE_NAME)\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {IMAGE_NAME} ...\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "        print(\"Done.\")\n",
    "    IMAGE_PATH = dest\n",
    "else:\n",
    "    IMAGE_PATH = os.path.join(\"..\", IMAGE_DIR, IMAGE_NAME)\n",
    "\n",
    "# Grayscale image (saved in Week 2; regenerate if missing)\n",
    "GRAY_PATH = os.path.join(\"..\", \"week2\", \"parrots_gray.jpg\")\n",
    "if not os.path.exists(GRAY_PATH):\n",
    "    GRAY_PATH = \"parrots_gray.jpg\"\n",
    "    _gray = np.array(Image.open(IMAGE_PATH).convert(\"L\"))\n",
    "    Image.fromarray(_gray).save(GRAY_PATH)\n",
    "    print(f\"Generated grayscale image at {GRAY_PATH}\")\n",
    "\n",
    "print(f\"Image path: {IMAGE_PATH}\")\n",
    "print(f\"Running on: {'Google Colab' if IN_COLAB else 'Local'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display helpers\n",
    "\n",
    "We define utility functions used throughout this notebook. This week adds `show_kernel` for visualizing filter weights and `show_spectrum` for frequency domain analysis, alongside the familiar `show_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def show_images(*imgs, titles=None, scale=4):\n    \"\"\"Display images in a single row.\n\n    Grayscale (2-D) arrays automatically use a gray colormap.\n    *titles* is an optional list of strings, one per image.\n    \"\"\"\n    n = len(imgs)\n    fig, axes = plt.subplots(1, n, figsize=(scale * n, scale))\n    if n == 1:\n        axes = [axes]\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        if img.ndim == 2:\n            ax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n        else:\n            ax.imshow(img)\n        if titles:\n            ax.set_title(titles[i])\n        ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_proportional(*imgs, titles=None, scale=6):\n    \"\"\"Display images vertically, each at its proportional size.\n\n    *scale* sets the width (in inches) of the widest image.\n    \"\"\"\n    n = len(imgs)\n    max_w = max(img.shape[1] for img in imgs)\n    px_per_inch = max_w / scale\n    heights = [img.shape[0] / px_per_inch for img in imgs]\n    widths = [img.shape[1] / px_per_inch for img in imgs]\n    total_h = sum(heights) + 0.5 * (n - 1)\n\n    fig, axes = plt.subplots(n, 1, figsize=(scale, total_h),\n                             gridspec_kw={\"height_ratios\": [img.shape[0] for img in imgs]})\n    if n == 1:\n        axes = [axes]\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        ax.set_anchor(\"W\")          # left-align each subplot\n        if img.ndim == 2:\n            ax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n        else:\n            ax.imshow(img)\n        if titles:\n            ax.set_title(titles[i], loc=\"left\")\n        ax.axis(\"off\")\n    fig.subplots_adjust(hspace=0.05)\n    plt.show()\n\n\ndef show_kernel(kernel, title=\"Kernel\", ax=None):\n    \"\"\"Display a small kernel as an annotated heatmap.\"\"\"\n    standalone = ax is None\n    if standalone:\n        fig, ax = plt.subplots(figsize=(3, 3))\n    vmax = np.max(np.abs(kernel))\n    ax.imshow(kernel, cmap=\"coolwarm\", vmin=-vmax, vmax=vmax)\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            val = kernel[i, j]\n            text = f\"{val:.2f}\" if val != int(val) else str(int(val))\n            ax.text(j, i, text, ha=\"center\", va=\"center\", fontsize=10)\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if standalone:\n        plt.tight_layout()\n        plt.show()\n\n\ndef show_spectrum(img, title=\"\"):\n    \"\"\"Display a grayscale image and its centered log-magnitude spectrum side by side.\"\"\"\n    F = np.fft.fft2(img.astype(np.float64))\n    F_shifted = np.fft.fftshift(F)\n    magnitude = np.log1p(np.abs(F_shifted))\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n    axes[0].imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n    axes[0].set_title(f\"{title} (Spatial)\" if title else \"Spatial domain\")\n    axes[0].axis(\"off\")\n    axes[1].imshow(magnitude, cmap=\"gray\")\n    axes[1].set_title(f\"{title} (Spectrum)\" if title else \"Magnitude spectrum\")\n    axes[1].axis(\"off\")\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Image Resizing & Interpolation\n",
    "\n",
    "When we change an image’s dimensions, pixels must be **created** (upsampling) or **discarded** (downsampling). The interpolation method determines how new pixel values are estimated from existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_color = np.array(Image.open(IMAGE_PATH))\n",
    "img_gray = np.array(Image.open(GRAY_PATH))\n",
    "\n",
    "print(f\"Color \\u2014 shape: {img_color.shape}, dtype: {img_color.dtype}\")\n",
    "print(f\"Gray  \\u2014 shape: {img_gray.shape}, dtype: {img_gray.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Downsampling by pixel skipping\n\nThe simplest downsampling: take every $N$th pixel and discard the rest. This is just NumPy slicing: `img[::N, ::N]`.\n\nEach image below is displayed at its **actual relative size** so you can see how much information is lost."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "factors = [1, 2, 4, 8]\ndownsampled = [img_gray[::f, ::f] for f in factors]\ntitles = [f\"{d.shape[0]}×{d.shape[1]}\" if f == 1\n          else f\"{d.shape[0]}×{d.shape[1]} (1/{f})\"\n          for f, d in zip(factors, downsampled)]\n\nshow_proportional(*downsampled, titles=titles)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling with different interpolation methods\n",
    "\n",
    "To upsample, we must **estimate** new pixel values. Three common methods:\n",
    "- **Nearest neighbor**: copy the closest pixel — fast but blocky\n",
    "- **Bilinear**: weighted average of 4 neighbors — smooth but slightly blurry\n",
    "- **Bicubic**: cubic polynomial through 16 neighbors — sharpest, most common default\n",
    "\n",
    "Let’s downsample to a small size, then upsample back to see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_gray = Image.open(GRAY_PATH)\n",
    "H, W = img_gray.shape\n",
    "\n",
    "# Downsample to 64\\u00d764\n",
    "small = pil_gray.resize((64, 64), Image.NEAREST)\n",
    "\n",
    "# Upsample back with different methods\n",
    "methods = {\"Nearest\": Image.NEAREST, \"Bilinear\": Image.BILINEAR, \"Bicubic\": Image.BICUBIC}\n",
    "upsampled = {name: np.array(small.resize((W, H), method)) for name, method in methods.items()}\n",
    "\n",
    "show_images(img_gray, upsampled[\"Nearest\"], upsampled[\"Bilinear\"], upsampled[\"Bicubic\"],\n",
    "            titles=[\"Original\", \"Nearest\", \"Bilinear\", \"Bicubic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s zoom in on a small region to see the interpolation artifacts more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1, r2, c1, c2 = 150, 300, 150, 300\n",
    "crops = [img_gray[r1:r2, c1:c2]] + [upsampled[m][r1:r2, c1:c2] for m in methods]\n",
    "\n",
    "show_images(*crops, titles=[\"Original\"] + list(methods.keys()), scale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**Exercise 1.1:** Downsample the **color** image by a factor of 4 using pixel skipping (`img_color[::4, ::4]`), then upsample it back to the original size using PIL with `Image.BICUBIC`. Display the original and the result side by side. What details are lost?\n",
    "\n",
    "*Steps:*\n",
    "1. Downsample: `small = img_color[::4, ::4]`\n",
    "2. Convert to PIL: `Image.fromarray(small)`\n",
    "3. Resize back: `.resize((W, H), Image.BICUBIC)` where `H, W = img_color.shape[:2]`\n",
    "4. Convert to array: `np.array(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2:** Compare the histograms of the original grayscale image and the nearest-neighbor upsampled version (from the demo above). Plot them overlaid on the same axes using `plt.fill_between`. What do you notice about the histogram shape?\n",
    "\n",
    "*Hint:* This connects to **Week 2 Section 7 (Histograms)**. Compute histograms with `np.histogram(img.ravel(), 256, [0, 256])` and plot with `plt.fill_between`. The nearest-neighbor histogram should look like a “comb” pattern because only certain pixel values are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Spatial Filtering & Convolution\n",
    "\n",
    "In Week 2, we transformed each pixel **independently** (point processing: $g(x,y) = T[f(x,y)]$). Now we use **neighborhoods**: the output at each pixel depends on the pixel *and its surrounding region*.\n",
    "\n",
    "The core operation is **convolution**: slide a small weight matrix (kernel) over the image, computing a weighted sum at each position.\n",
    "\n",
    "$$g(x,y) = \\sum_{s=-k}^{k}\\sum_{t=-k}^{k} h(s,t) \\cdot f(x-s,\\, y-t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D convolution: warm-up\n",
    "\n",
    "Before tackling 2D images, let’s see convolution on a 1D signal. The steps are:\n",
    "1. **Flip** the kernel\n",
    "2. **Slide** it across the signal\n",
    "3. At each position, **multiply** element-wise and **sum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = np.array([0, 0, 1, 2, 3, 2, 1, 0, 0], dtype=np.float64)\n",
    "kernel_1d = np.array([1, 0, -1], dtype=np.float64)\n",
    "\n",
    "# Manual 1D convolution (valid mode)\n",
    "out_len = len(signal) - len(kernel_1d) + 1\n",
    "output = np.zeros(out_len)\n",
    "k_flip = kernel_1d[::-1]\n",
    "for i in range(out_len):\n",
    "    output[i] = np.sum(signal[i:i + len(kernel_1d)] * k_flip)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "axes[0].stem(signal)\n",
    "axes[0].set_title(\"Signal\")\n",
    "axes[1].stem(kernel_1d)\n",
    "axes[1].set_title(\"Kernel [1, 0, -1]\")\n",
    "axes[2].stem(output)\n",
    "axes[2].set_title(\"Convolution output (valid)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D convolution: step by step\n",
    "\n",
    "Now let’s extend to 2D. We’ll work through a small example: a 5×5 “image” convolved with a 3×3 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_img = np.array([\n",
    "    [1,  2,  3,  4,  5],\n",
    "    [6,  7,  8,  9,  10],\n",
    "    [11, 12, 13, 14, 15],\n",
    "    [16, 17, 18, 19, 20],\n",
    "    [21, 22, 23, 24, 25]\n",
    "], dtype=np.float64)\n",
    "\n",
    "kernel_2d = np.array([\n",
    "    [0, -1,  0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1,  0]\n",
    "], dtype=np.float64)\n",
    "\n",
    "print(f\"Image (5\\u00d75):\\n{small_img.astype(int)}\")\n",
    "print()\n",
    "show_kernel(kernel_2d, title=\"Kernel (3\\u00d73)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compute one output pixel explicitly: extract the 3×3 neighborhood, flip the kernel 180°, multiply element-wise, and sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convolution: flip the kernel 180 degrees\n",
    "k_flip = kernel_2d[::-1, ::-1]\n",
    "\n",
    "# Compute output at position (1, 1) \\u2014 kernel centered at row=1, col=1\n",
    "row, col = 1, 1\n",
    "neighborhood = small_img[row:row + 3, col:col + 3]\n",
    "products = neighborhood * k_flip\n",
    "\n",
    "print(f\"Neighborhood at ({row},{col}):\\n{neighborhood.astype(int)}\")\n",
    "print(f\"\\nFlipped kernel:\\n{k_flip.astype(int)}\")\n",
    "print(f\"\\nElement-wise products:\\n{products.astype(int)}\")\n",
    "print(f\"\\nSum = {products.sum():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing 2D convolution from scratch\n",
    "\n",
    "Let’s write a full convolution function. This builds deep intuition for what every library function does under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(image, kernel, mode=\"same\"):\n",
    "    \"\"\"2D convolution (not correlation) of a grayscale image with a kernel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image  : 2D array (H, W)\n",
    "    kernel : 2D array (kH, kW), must be odd-sized\n",
    "    mode   : \"valid\" or \"same\"\n",
    "    \"\"\"\n",
    "    img = image.astype(np.float64)\n",
    "    k = kernel[::-1, ::-1].astype(np.float64)   # flip for true convolution\n",
    "    kH, kW = k.shape\n",
    "\n",
    "    if mode == \"same\":\n",
    "        pH, pW = kH // 2, kW // 2\n",
    "        img = np.pad(img, ((pH, pH), (pW, pW)), mode=\"constant\", constant_values=0)\n",
    "    elif mode != \"valid\":\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    H, W = img.shape\n",
    "    outH, outW = H - kH + 1, W - kW + 1\n",
    "    output = np.zeros((outH, outW), dtype=np.float64)\n",
    "\n",
    "    for i in range(outH):\n",
    "        for j in range(outW):\n",
    "            output[i, j] = np.sum(img[i:i + kH, j:j + kW] * k)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_valid = conv2d(small_img, kernel_2d, mode=\"valid\")\n",
    "result_same = conv2d(small_img, kernel_2d, mode=\"same\")\n",
    "\n",
    "print(f\"Input shape:        {small_img.shape}\")\n",
    "print(f\"Kernel shape:       {kernel_2d.shape}\")\n",
    "print(f\"Valid output shape: {result_valid.shape}  (5 - 3 + 1 = 3)\")\n",
    "print(f\"Same output shape:  {result_same.shape}   (padded to match input)\")\n",
    "print(f\"\\nValid output:\\n{result_valid.astype(int)}\")\n",
    "print(f\"\\nSame output:\\n{result_same.astype(int)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution vs Correlation\n",
    "\n",
    "**Convolution** flips the kernel 180° before sliding: $g(x,y) = \\sum h(s,t) \\cdot f(x\\mathbf{-}s,\\, y\\mathbf{-}t)$\n",
    "\n",
    "**Correlation** uses the kernel as-is: $g(x,y) = \\sum h(s,t) \\cdot f(x\\mathbf{+}s,\\, y\\mathbf{+}t)$\n",
    "\n",
    "For **symmetric** kernels (like box and Gaussian filters), they produce identical results. Most libraries (including `scipy.ndimage.correlate`) compute **correlation** but call it “convolution” by convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our implementation (true convolution)\n",
    "ours = conv2d(small_img, kernel_2d, mode=\"same\")\n",
    "\n",
    "# scipy.signal.convolve2d (true convolution)\n",
    "scipy_conv = convolve2d(small_img, kernel_2d, mode=\"same\", boundary=\"fill\", fillvalue=0)\n",
    "\n",
    "# scipy.ndimage.correlate (correlation)\n",
    "scipy_corr = correlate(small_img, kernel_2d, mode=\"constant\", cval=0)\n",
    "\n",
    "print(f\"Our conv2d:\\n{ours.astype(int)}\")\n",
    "print(f\"\\nscipy convolve2d:\\n{scipy_conv.astype(int)}\")\n",
    "print(f\"\\nscipy correlate:\\n{scipy_corr.astype(int)}\")\n",
    "print(f\"\\nOurs matches convolve2d: {np.allclose(ours, scipy_conv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying convolution to a real image\n",
    "\n",
    "Let’s apply the sharpening kernel to the grayscale parrot image.\n",
    "\n",
    "> Since our from-scratch implementation uses Python loops, it is **slow** on full-size images. For the rest of this notebook we use `scipy.ndimage.correlate` for efficiency (for symmetric kernels, correlation = convolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpened = correlate(img_gray.astype(np.float64), kernel_2d, mode=\"constant\", cval=0)\n",
    "sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "\n",
    "show_images(img_gray, sharpened, titles=[\"Original\", \"Sharpened (kernel from above)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output size\n",
    "\n",
    "The output dimensions depend on the padding mode:\n",
    "\n",
    "| Mode | Padding $p$ | Output size |\n",
    "|:---:|:---:|:---:|\n",
    "| **Valid** | $0$ | $H - F + 1$ |\n",
    "| **Same** | $\\lfloor F/2 \\rfloor$ | $H$ |\n",
    "| **Full** | $F - 1$ | $H + F - 1$ |\n",
    "\n",
    "General formula: $O = H + 2p - F + 1$, where $F$ is the kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = img_gray.shape\n",
    "F = 5\n",
    "test_kernel = np.ones((F, F)) / (F * F)\n",
    "\n",
    "valid_out = convolve2d(img_gray, test_kernel, mode=\"valid\")\n",
    "same_out  = convolve2d(img_gray, test_kernel, mode=\"same\")\n",
    "full_out  = convolve2d(img_gray, test_kernel, mode=\"full\")\n",
    "\n",
    "print(f\"Input:  {img_gray.shape}\")\n",
    "print(f\"Kernel: {test_kernel.shape}\")\n",
    "print(f\"Valid:  {valid_out.shape}  (expected {H - F + 1}\\u00d7{W - F + 1})\")\n",
    "print(f\"Same:   {same_out.shape}  (expected {H}\\u00d7{W})\")\n",
    "print(f\"Full:   {full_out.shape}  (expected {H + F - 1}\\u00d7{W + F - 1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**Exercise 2.1:** Implement a **correlation** function `corr2d(image, kernel, mode=\"same\")` by modifying the `conv2d` function above. The only difference: do **not** flip the kernel.\n",
    "\n",
    "Test it on `small_img` with `kernel_2d` and verify that:\n",
    "- Your `corr2d` matches `scipy.ndimage.correlate`\n",
    "- For the **symmetric** identity kernel $\\begin{bmatrix}0&0&0\\\\0&1&0\\\\0&0&0\\end{bmatrix}$, both `conv2d` and `corr2d` produce the same result (the original image)\n",
    "\n",
    "*Hint:* Copy `conv2d`, remove the line `k = kernel[::-1, ::-1]...`, and use the kernel directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2:** Use `scipy.ndimage.correlate` to apply the **identity kernel** $\\begin{bmatrix}0&0&0\\\\0&1&0\\\\0&0&0\\end{bmatrix}$ to the grayscale image. Display the original and the result side by side — they should look identical.\n",
    "\n",
    "Then apply a **shift kernel** $\\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{bmatrix}$ and observe the effect. What happens to the image?\n",
    "\n",
    "*Hint:* Create the identity kernel with `K = np.array([[0,0,0],[0,1,0],[0,0,0]])`. The shift kernel places the `1` at a different position, which shifts the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Smoothing Filters\n",
    "\n",
    "Smoothing filters blur the image by averaging each pixel with its neighbors. They are **low-pass** filters: they preserve gradual intensity changes (low frequencies) and suppress rapid changes like edges and noise (high frequencies).\n",
    "\n",
    "Two main types:\n",
    "- **Box filter**: all weights equal (simple average)\n",
    "- **Gaussian filter**: closer neighbors get higher weight (bell curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box filter\n",
    "\n",
    "The simplest smoothing kernel: every weight is $\\frac{1}{N^2}$. A 3×3 box filter:\n",
    "\n",
    "$$h = \\frac{1}{9}\\begin{bmatrix}1 & 1 & 1\\\\1 & 1 & 1\\\\1 & 1 & 1\\end{bmatrix}$$\n",
    "\n",
    "Each output pixel becomes the **average** of its $3 \\times 3$ neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_box_kernel(size):\n",
    "    \"\"\"Create a box (mean) kernel of the given size.\"\"\"\n",
    "    return np.ones((size, size), dtype=np.float64) / (size * size)\n",
    "\n",
    "box3 = make_box_kernel(3)\n",
    "show_kernel(box3, title=\"3\\u00d73 Box kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [3, 7, 15, 31]\n",
    "blurred = []\n",
    "for s in sizes:\n",
    "    k = make_box_kernel(s)\n",
    "    b = correlate(img_gray.astype(np.float64), k, mode=\"constant\", cval=0)\n",
    "    blurred.append(np.clip(b, 0, 255).astype(np.uint8))\n",
    "\n",
    "show_images(img_gray, *blurred,\n",
    "            titles=[\"Original\"] + [f\"Box {s}\\u00d7{s}\" for s in sizes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian filter\n",
    "\n",
    "Instead of equal weights, a Gaussian kernel gives **higher weight to closer neighbors**:\n",
    "\n",
    "$$h(s,t) = \\frac{1}{2\\pi\\sigma^2}\\exp\\!\\left(-\\frac{s^2+t^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "The parameter $\\sigma$ controls the spread: larger $\\sigma$ = wider kernel = more smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gaussian_kernel(size, sigma):\n",
    "    \"\"\"Create a 2D Gaussian kernel of the given size and sigma.\"\"\"\n",
    "    k = size // 2\n",
    "    ax = np.arange(-k, k + 1, dtype=np.float64)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    kernel = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    kernel /= kernel.sum()\n",
    "    return kernel\n",
    "\n",
    "g5 = make_gaussian_kernel(5, sigma=1.0)\n",
    "print(f\"Kernel shape: {g5.shape}\")\n",
    "print(f\"Sum of weights: {g5.sum():.6f}\")\n",
    "show_kernel(g5, title=\"5\\u00d75 Gaussian (\\u03c3=1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g15 = make_gaussian_kernel(15, sigma=3.0)\n",
    "ax_range = np.arange(-7, 8)\n",
    "xx, yy = np.meshgrid(ax_range, ax_range)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(xx, yy, g15, cmap=\"viridis\")\n",
    "ax.set_title(\"Gaussian kernel (\\u03c3=3.0)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [1, 3, 5, 10]\n",
    "gaussian_blurred = []\n",
    "for sigma in sigmas:\n",
    "    size = int(6 * sigma + 1) | 1   # ensure odd size, ~6\\u03c3 wide\n",
    "    k = make_gaussian_kernel(size, sigma)\n",
    "    b = correlate(img_gray.astype(np.float64), k, mode=\"constant\", cval=0)\n",
    "    gaussian_blurred.append(np.clip(b, 0, 255).astype(np.uint8))\n",
    "\n",
    "show_images(img_gray, *gaussian_blurred,\n",
    "            titles=[\"Original\"] + [f\"Gaussian \\u03c3={s}\" for s in sigmas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box vs Gaussian comparison\n",
    "\n",
    "At similar kernel sizes, Gaussian produces a **smoother, more natural** blur because nearby pixels contribute more than distant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_result = correlate(img_gray.astype(np.float64), make_box_kernel(15), mode=\"constant\", cval=0)\n",
    "gauss_result = correlate(img_gray.astype(np.float64), make_gaussian_kernel(15, 3.0), mode=\"constant\", cval=0)\n",
    "\n",
    "box_result = np.clip(box_result, 0, 255).astype(np.uint8)\n",
    "gauss_result = np.clip(gauss_result, 0, 255).astype(np.uint8)\n",
    "\n",
    "show_images(img_gray, box_result, gauss_result,\n",
    "            titles=[\"Original\", \"Box 15\\u00d715\", \"Gaussian 15\\u00d715 (\\u03c3=3)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**Exercise 3.1:** Apply a 7×7 box filter to the **color** image. Process each channel (R, G, B) independently, then recombine into a color image.\n",
    "\n",
    "*Steps:*\n",
    "1. Split channels: `r, g, b = img_color[:,:,0], img_color[:,:,1], img_color[:,:,2]`\n",
    "2. Apply `correlate` to each channel (cast to `float64`, use `make_box_kernel(7)`)\n",
    "3. Clip each to [0, 255] and cast to `uint8`\n",
    "4. Stack: `result = np.stack([r_blur, g_blur, b_blur], axis=2)`\n",
    "5. Display original and blurred with `show_images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2:** Smooth the grayscale image with a Gaussian filter ($\\sigma=3$), then compute the **difference** between the original and the smoothed image: `diff = original - blurred`. This difference is the **high-frequency detail** that smoothing removes.\n",
    "\n",
    "Display three images: original, smoothed, and the difference (rescaled for visibility).\n",
    "\n",
    "*Hint:* The difference image may contain negative values. To display it, rescale to [0, 255]:\n",
    "```python\n",
    "diff = img_gray.astype(np.float64) - blurred.astype(np.float64)\n",
    "diff_display = ((diff - diff.min()) / (diff.max() - diff.min()) * 255).astype(np.uint8)\n",
    "```\n",
    "\n",
    "This is the same min-max rescaling from **Week 2 Section 8 (Histogram Stretching)** — we stretch the difference image to the full [0, 255] range for display. This difference image also foreshadows the sharpening concept in Section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Frequency Domain & 2D FFT\n",
    "\n",
    "The **Convolution Theorem** says: convolution in the spatial domain equals **multiplication** in the frequency domain.\n",
    "\n",
    "$$f * h \\;\\longleftrightarrow\\; F \\cdot H$$\n",
    "\n",
    "The **2D Discrete Fourier Transform (DFT)** decomposes an image into its constituent spatial frequencies. Low frequencies represent smooth, gradual changes; high frequencies represent edges and fine detail.\n",
    "\n",
    "Key NumPy functions:\n",
    "- `np.fft.fft2(img)` — compute the 2D DFT\n",
    "- `np.fft.fftshift(F)` — move the DC component (average brightness) to the center\n",
    "- `np.fft.ifft2(F)` — inverse DFT (back to spatial domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude spectrum\n",
    "\n",
    "To visualize the frequency content, we plot the **log-magnitude** of the DFT: $\\log(1 + |F(u,v)|)$.\n",
    "\n",
    "The log scaling compresses the huge dynamic range so that both the bright DC component and the faint high-frequency components are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrum(img_gray, title=\"Grayscale parrot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How image content maps to frequency content\n",
    "\n",
    "Smooth regions concentrate energy at the **center** (low frequencies), while edges and textures spread energy **outward** (high frequencies). Let’s compare the original with a heavily blurred version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = correlate(img_gray.astype(np.float64),\n",
    "                   make_gaussian_kernel(31, 5.0), mode=\"constant\", cval=0)\n",
    "smooth = np.clip(smooth, 0, 255).astype(np.uint8)\n",
    "\n",
    "show_spectrum(img_gray, title=\"Original\")\n",
    "show_spectrum(smooth, title=\"Heavily smoothed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-pass filtering in frequency domain\n",
    "\n",
    "We can blur an image directly in the frequency domain:\n",
    "1. Compute 2D DFT\n",
    "2. Multiply by a **low-pass mask** (keeps center, blocks edges)\n",
    "3. Inverse DFT back to spatial domain\n",
    "\n",
    "The result is equivalent to spatial convolution with a smoothing filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 2D DFT\n",
    "F = np.fft.fft2(img_gray.astype(np.float64))\n",
    "F_shifted = np.fft.fftshift(F)\n",
    "\n",
    "# Create a circular low-pass mask\n",
    "H, W = img_gray.shape\n",
    "cy, cx = H // 2, W // 2\n",
    "Y, X = np.ogrid[:H, :W]\n",
    "radius = 30\n",
    "mask_lp = ((X - cx)**2 + (Y - cy)**2 <= radius**2).astype(np.float64)\n",
    "\n",
    "# Apply mask and inverse FFT\n",
    "F_filtered = F_shifted * mask_lp\n",
    "result_lp = np.fft.ifft2(np.fft.ifftshift(F_filtered)).real\n",
    "result_lp = np.clip(result_lp, 0, 255).astype(np.uint8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "axes[0].imshow(img_gray, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[1].imshow(np.log1p(np.abs(F_shifted)), cmap=\"gray\")\n",
    "axes[1].set_title(\"Spectrum\")\n",
    "axes[2].imshow(mask_lp, cmap=\"gray\")\n",
    "axes[2].set_title(f\"Low-pass mask (r={radius})\")\n",
    "axes[3].imshow(result_lp, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axes[3].set_title(\"Low-pass filtered\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-pass filtering in frequency domain\n",
    "\n",
    "The complement of low-pass is high-pass: $H_{\\text{HP}} = 1 - H_{\\text{LP}}$\n",
    "\n",
    "High-pass filtering keeps edges and fine detail while removing smooth regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_hp = 1 - mask_lp\n",
    "\n",
    "F_hp = F_shifted * mask_hp\n",
    "result_hp = np.fft.ifft2(np.fft.ifftshift(F_hp)).real\n",
    "\n",
    "# Rescale for display (high-pass output has negative values)\n",
    "hp_min, hp_max = result_hp.min(), result_hp.max()\n",
    "result_hp_display = ((result_hp - hp_min) / (hp_max - hp_min) * 255).astype(np.uint8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(img_gray, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[1].imshow(mask_hp, cmap=\"gray\")\n",
    "axes[1].set_title(\"High-pass mask\")\n",
    "axes[2].imshow(result_hp_display, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axes[2].set_title(\"High-pass filtered (edges)\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency response of spatial kernels\n",
    "\n",
    "We can compute the frequency response of any spatial kernel by zero-padding it to image size and taking its DFT. This reveals **why Gaussian beats Box**:\n",
    "\n",
    "- **Gaussian**: smooth, clean rolloff in frequency (no oscillations)\n",
    "- **Box**: sinc-like response with sidelobes that cause **ringing artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_freq_response(kernel, shape):\n",
    "    \"\"\"Zero-pad kernel to shape and return centered magnitude spectrum.\"\"\"\n",
    "    padded = np.zeros(shape, dtype=np.float64)\n",
    "    kH, kW = kernel.shape\n",
    "    padded[:kH, :kW] = kernel\n",
    "    F = np.fft.fftshift(np.fft.fft2(padded))\n",
    "    return np.abs(F)\n",
    "\n",
    "box_k = make_box_kernel(15)\n",
    "gauss_k = make_gaussian_kernel(15, sigma=3.0)\n",
    "\n",
    "box_resp = kernel_freq_response(box_k, img_gray.shape)\n",
    "gauss_resp = kernel_freq_response(gauss_k, img_gray.shape)\n",
    "\n",
    "# 1D cross-section through center\n",
    "center = img_gray.shape[0] // 2\n",
    "freq_axis = np.arange(img_gray.shape[1]) - img_gray.shape[1] // 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(freq_axis, box_resp[center, :], label=\"Box 15\\u00d715\")\n",
    "axes[0].plot(freq_axis, gauss_resp[center, :], label=\"Gaussian \\u03c3=3\")\n",
    "axes[0].set_title(\"Frequency response (1D cross-section)\")\n",
    "axes[0].set_xlabel(\"Frequency\")\n",
    "axes[0].set_ylabel(\"Magnitude\")\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim([-100, 100])\n",
    "\n",
    "axes[1].imshow(np.log1p(box_resp), cmap=\"gray\")\n",
    "axes[1].set_title(\"Box 2D response (log)\")\n",
    "axes[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**Exercise 4.1:** Experiment with different **low-pass cutoff radii** (10, 30, 60, 100). For each radius, create a circular mask, apply it in the frequency domain, and display the result. Show all four filtered images in a single row.\n",
    "\n",
    "*Steps:*\n",
    "1. Compute `F_shifted = np.fft.fftshift(np.fft.fft2(img_gray.astype(np.float64)))`\n",
    "2. For each radius `r`, create a mask: `((X - cx)**2 + (Y - cy)**2 <= r**2).astype(np.float64)` — reuse `Y, X, cy, cx` from the demo above\n",
    "3. Filter: `np.fft.ifft2(np.fft.ifftshift(F_shifted * mask)).real`\n",
    "4. Clip to [0, 255] and cast to `uint8`\n",
    "\n",
    "What happens as the radius increases? How does this relate to the kernel size of spatial smoothing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.2:** Compute the magnitude spectrum of the **original** grayscale image and of a **gamma-corrected** version ($\\gamma = 0.3$). Display both spectra side by side using `show_spectrum`.\n",
    "\n",
    "*Hint:* Apply gamma correction from Week 2: `corrected = (255 * (img_gray / 255.0) ** 0.3).astype(np.uint8)`. How does brightening dark regions affect the frequency content?\n",
    "\n",
    "This connects to **Week 2 Section 5 (Gamma Correction)** — you are observing gamma’s effect in the frequency domain, not just the spatial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Sharpening Filters\n",
    "\n",
    "Sharpening enhances edges and fine detail. The key insight:\n",
    "\n",
    "$$\\text{High-pass} = \\text{Original} - \\text{Low-pass (blurred)}$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\text{Sharpened} = \\text{Original} + \\alpha \\cdot \\text{High-pass}$$\n",
    "\n",
    "Two common approaches:\n",
    "1. **Laplacian**: a single kernel that directly computes the second derivative (edges)\n",
    "2. **Unsharp masking**: blur the image, subtract the blur to get edges, add them back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian filter\n",
    "\n",
    "The Laplacian kernel detects edges by computing the **second derivative** of the image. Its weights sum to **zero**, so flat regions produce zero output — only intensity changes survive.\n",
    "\n",
    "Two variants:\n",
    "- **4-connected**: $\\begin{bmatrix}0 & 1 & 0\\\\1 & -4 & 1\\\\0 & 1 & 0\\end{bmatrix}$ — horizontal & vertical edges\n",
    "- **8-connected**: $\\begin{bmatrix}1 & 1 & 1\\\\1 & -8 & 1\\\\1 & 1 & 1\\end{bmatrix}$ — includes diagonal edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lap4 = np.array([[0,  1, 0],\n",
    "                 [1, -4, 1],\n",
    "                 [0,  1, 0]], dtype=np.float64)\n",
    "\n",
    "lap8 = np.array([[1,  1, 1],\n",
    "                 [1, -8, 1],\n",
    "                 [1,  1, 1]], dtype=np.float64)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "show_kernel(lap4, title=\"Laplacian (4-connected)\", ax=axes[0])\n",
    "show_kernel(lap8, title=\"Laplacian (8-connected)\", ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"4-connected sum: {lap4.sum():.0f}\")\n",
    "print(f\"8-connected sum: {lap8.sum():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges4 = correlate(img_gray.astype(np.float64), lap4, mode=\"constant\", cval=0)\n",
    "edges8 = correlate(img_gray.astype(np.float64), lap8, mode=\"constant\", cval=0)\n",
    "\n",
    "def rescale_for_display(img):\n",
    "    \"\"\"Rescale a float image to [0, 255] uint8 for display.\"\"\"\n",
    "    lo, hi = img.min(), img.max()\n",
    "    return ((img - lo) / (hi - lo) * 255).astype(np.uint8)\n",
    "\n",
    "show_images(img_gray, rescale_for_display(edges4), rescale_for_display(edges8),\n",
    "            titles=[\"Original\", \"Laplacian 4-connected\", \"Laplacian 8-connected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpening with the Laplacian\n",
    "\n",
    "To sharpen, **subtract** the Laplacian (edge map) from the original:\n",
    "\n",
    "$$\\text{Sharpened} = \\text{Original} - \\text{Laplacian}$$\n",
    "\n",
    "(We subtract because the standard Laplacian has a **negative** center weight. Equivalently, some formulations negate the kernel and add instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpened_lap = img_gray.astype(np.float64) - edges4\n",
    "sharpened_lap = np.clip(sharpened_lap, 0, 255).astype(np.uint8)\n",
    "\n",
    "show_images(img_gray, rescale_for_display(edges4), sharpened_lap,\n",
    "            titles=[\"Original\", \"Laplacian edges\", \"Sharpened (orig \\u2212 Laplacian)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsharp masking\n",
    "\n",
    "A more flexible sharpening method with a tunable strength parameter $\\alpha$:\n",
    "\n",
    "$$\\text{Sharpened} = \\text{Original} + \\alpha \\cdot (\\text{Original} - \\text{Blurred})$$\n",
    "\n",
    "The term $(\\text{Original} - \\text{Blurred})$ extracts the high-frequency detail. Adding it back at strength $\\alpha$ enhances edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsharp_mask(img, sigma=3.0, alpha=1.0):\n",
    "    \"\"\"Apply unsharp masking to a grayscale image.\"\"\"\n",
    "    size = int(6 * sigma + 1) | 1\n",
    "    kernel = make_gaussian_kernel(size, sigma)\n",
    "    blurred = correlate(img.astype(np.float64), kernel, mode=\"constant\", cval=0)\n",
    "    detail = img.astype(np.float64) - blurred\n",
    "    sharpened = img.astype(np.float64) + alpha * detail\n",
    "    return np.clip(sharpened, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.5, 1.0, 2.0, 4.0]\n",
    "results = [unsharp_mask(img_gray, sigma=3.0, alpha=a) for a in alphas]\n",
    "\n",
    "show_images(img_gray, *results,\n",
    "            titles=[\"Original\"] + [f\"\\u03b1 = {a}\" for a in alphas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also visualize the **detail** (high-pass) image that unsharp masking extracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(6 * 3.0 + 1) | 1\n",
    "k = make_gaussian_kernel(size, 3.0)\n",
    "blurred_for_detail = correlate(img_gray.astype(np.float64), k, mode=\"constant\", cval=0)\n",
    "detail = img_gray.astype(np.float64) - blurred_for_detail\n",
    "\n",
    "show_images(img_gray,\n",
    "            np.clip(blurred_for_detail, 0, 255).astype(np.uint8),\n",
    "            rescale_for_display(detail),\n",
    "            titles=[\"Original\", \"Blurred (low-pass)\", \"Detail (high-pass)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**Exercise 5.1:** Apply **unsharp masking** to the **color** image. Process each channel independently using the `unsharp_mask` function with $\\sigma=2, \\alpha=1.5$. Recombine the channels and display original vs sharpened.\n",
    "\n",
    "*Steps:*\n",
    "1. Split: `channels = [img_color[:,:,c] for c in range(3)]`\n",
    "2. Apply: `sharp_channels = [unsharp_mask(ch, sigma=2.0, alpha=1.5) for ch in channels]`\n",
    "3. Stack: `result = np.stack(sharp_channels, axis=2)`\n",
    "4. Display with `show_images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.2:** Create a **before/after comparison** that combines Weeks 2 and 3:\n",
    "1. Start with the grayscale image\n",
    "2. Apply **gamma correction** with $\\gamma = 0.7$ to brighten dark regions (Week 2)\n",
    "3. Apply **unsharp masking** with $\\sigma=2, \\alpha=1.0$ to sharpen (Week 3)\n",
    "4. Display three images: Original, After gamma, After gamma + sharpening\n",
    "\n",
    "*Hint:* Apply gamma first: `bright = (255 * (img_gray / 255.0) ** 0.7).astype(np.uint8)`, then sharpen with `unsharp_mask(bright, sigma=2.0, alpha=1.0)`.\n",
    "\n",
    "This demonstrates a realistic **image enhancement pipeline**: first fix brightness, then sharpen details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.3:** Apply the 8-connected Laplacian kernel to the grayscale image and display the edge map. Then compute the **histogram** of the edge image (before rescaling) and plot it.\n",
    "\n",
    "What is the shape of this histogram? Why does it look so different from the histogram of the original image?\n",
    "\n",
    "*Hint:* Use `correlate` with `lap8`. For the histogram, work with the raw float output. Cast to `int16` first:\n",
    "```python\n",
    "edges = correlate(img_gray.astype(np.float64), lap8, mode=\"constant\", cval=0)\n",
    "edges_int = edges.astype(np.int16)\n",
    "h, bins = np.histogram(edges_int.ravel(), bins=100)\n",
    "plt.fill_between(bins[:-1], h, alpha=0.7)\n",
    "```\n",
    "\n",
    "This connects to **Week 2 Section 7 (Histograms)** — you are analyzing histogram shape to understand what a filter does to the intensity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ]
}